{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMifCEiFZvrpTXNU5n3c6vW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PhillDev-coder256/Understanding-Machine-Learning-Frameworks/blob/main/Understanding_Machine_Learning_Frameworks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Getting started with Tensorflow in Google Colab**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "MnBYK8BaPNAw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting Started with TensorFlow in Google Colab\n",
        "Google Colab is an excellent platform to run TensorFlow without needing to install anything locally. Follow these steps to get started:\n",
        "\n",
        "Step 1: Open Google Colab\n",
        "Visit Google Colab and sign in with your Google account if you haven't already.\n",
        "\n",
        "Step 2: Create a New Notebook\n",
        "Click on File > New Notebook to create a new Colab notebook.\n",
        "\n",
        "Step 3: Verify TensorFlow Installation\n",
        "Google Colab comes with TensorFlow pre-installed. You can verify the installation by running the following code:"
      ],
      "metadata": {
        "id": "KYtTxpHzLqlh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FM29cULSH2JG",
        "outputId": "93e90bad-053e-48db-dcb9-93772c19e1ca"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Start Using TensorFlow\n",
        "Now you can start using TensorFlow in your Colab notebook. Here's an example of how to create a simple neural network:"
      ],
      "metadata": {
        "id": "3Gk1E0_tL4Sm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VVQngLqHcvN",
        "outputId": "143bec88-1494-4d98-c15f-2232ff04ac95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_2 (Dense)             (None, 512)               401920    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 10)                5130      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 407050 (1.55 MB)\n",
            "Trainable params: 407050 (1.55 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Define a simple sequential model\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Dense(512, activation='relu', input_shape=(784,)),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.Dense(10)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Training a Model\n",
        "Here's an example of how to train the model using the MNIST dataset:"
      ],
      "metadata": {
        "id": "pKM-F23WMHCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and prepare the MNIST dataset\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Flatten the images\n",
        "x_train = x_train.reshape(-1, 784)\n",
        "x_test = x_test.reshape(-1, 784)\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "\n",
        "# Evaluate the model\n",
        "model.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dC5If7saMJYT",
        "outputId": "fd93007e-d492-4771-cde5-0d54df99be92"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 15s 8ms/step - loss: 0.2200 - accuracy: 0.9353\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 14s 8ms/step - loss: 0.0973 - accuracy: 0.9701\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0692 - accuracy: 0.9781\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0547 - accuracy: 0.9824\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0434 - accuracy: 0.9857\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0693 - accuracy: 0.9792\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.06926051527261734, 0.979200005531311]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Save and Load the Model\n",
        "You can save your trained model and load it later for inference or further training."
      ],
      "metadata": {
        "id": "dsEz8tNoMNI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "model.save('my_model.h5')\n",
        "\n",
        "# Load the model\n",
        "new_model = tf.keras.models.load_model('my_model.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dT117pSTMQEE",
        "outputId": "25a07b0c-3096-4620-c3f7-47ed4848a5ed"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the Model to Your Computer\n",
        "To download the saved model file to your local machine, you can use the following code:"
      ],
      "metadata": {
        "id": "8tbZhRdROEz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Download the model file\n",
        "files.download('my_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "2iV3Te0hOHlm",
        "outputId": "8d103119-44af-4512-d7d1-4f4809618924"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f0f06103-65d3-4243-bddd-77d99a8807fb\", \"my_model.h5\", 4911952)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model summary"
      ],
      "metadata": {
        "id": "uzeRS4_JN8x8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(new_model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bpBSyliNrjC",
        "outputId": "6341af92-4f70-4149-adc4-fbb7a5a95a01"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_2 (Dense)             (None, 512)               401920    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 10)                5130      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 407050 (1.55 MB)\n",
            "Trainable params: 407050 (1.55 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Additional Resources**\n",
        "\n",
        "TensorFlow Tutorials: Explore Tutorials\n",
        "TensorFlow Documentation: Read Documentation\n",
        "\n",
        "By following these steps, you can get started with TensorFlow in Google Colab and leverage its powerful tools and resources for your machine learning projects."
      ],
      "metadata": {
        "id": "wpZr0ToOMUL4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oc0IGvE2LpC7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Getting Started with Pytorch**"
      ],
      "metadata": {
        "id": "dkhO2cyNUiOL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is a step-by-step guide to creating a basic model in PyTorch, training it on the MNIST dataset, and saving/loading the model. We'll use Google Colab for this example.\n",
        "\n",
        "Step 1: Install PyTorch\n",
        "First, ensure you have PyTorch installed in your Colab environment:"
      ],
      "metadata": {
        "id": "UQp5e0xXUpq0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This installs PyTorch and torchvision, a library with commonly used datasets and transforms for computer vision.\n",
        "!pip install torch torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meW3N_smUtbm",
        "outputId": "19d881ac-438f-483e-99db-fcda4c8a8783"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Import Libraries\n",
        "Import the necessary libraries:"
      ],
      "metadata": {
        "id": "iotw8WX6UyLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "X3QpCgcEUzV3"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Load and Preprocess the Dataset\n",
        "Load the MNIST dataset and apply necessary transformations:"
      ],
      "metadata": {
        "id": "TeGlncnRU4u6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6oh00FnVU8Cm",
        "outputId": "65bdf039-4731-4b3b-c571-748c5b3d9ddf"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 12884446.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 347954.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 3197921.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 2460037.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Define the Model\n",
        "Define a simple neural network model:"
      ],
      "metadata": {
        "id": "8gqmgx89VAIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 512)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)  # Flatten the input\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = SimpleNN()"
      ],
      "metadata": {
        "id": "iyvzRXNPVBnb"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Define the Loss Function and Optimizer"
      ],
      "metadata": {
        "id": "Y_ky0XSGVFqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "1YcntLYQVKym"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Train the Model\n",
        "Train the model for a few epochs:"
      ],
      "metadata": {
        "id": "4h8v5rRxVOT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(5):  # loop over the dataset multiple times\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:  # print every 100 mini-batches\n",
        "            print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 100:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHF4kgfHVRXA",
        "outputId": "689c5b08-1a7e-4f35-e796-01b279050023"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1, Batch 100] loss: 0.818\n",
            "[Epoch 1, Batch 200] loss: 0.490\n",
            "[Epoch 1, Batch 300] loss: 0.367\n",
            "[Epoch 1, Batch 400] loss: 0.372\n",
            "[Epoch 1, Batch 500] loss: 0.331\n",
            "[Epoch 1, Batch 600] loss: 0.315\n",
            "[Epoch 1, Batch 700] loss: 0.255\n",
            "[Epoch 1, Batch 800] loss: 0.276\n",
            "[Epoch 1, Batch 900] loss: 0.242\n",
            "[Epoch 1, Batch 1000] loss: 0.214\n",
            "[Epoch 1, Batch 1100] loss: 0.249\n",
            "[Epoch 1, Batch 1200] loss: 0.218\n",
            "[Epoch 1, Batch 1300] loss: 0.192\n",
            "[Epoch 1, Batch 1400] loss: 0.187\n",
            "[Epoch 1, Batch 1500] loss: 0.203\n",
            "[Epoch 1, Batch 1600] loss: 0.177\n",
            "[Epoch 1, Batch 1700] loss: 0.172\n",
            "[Epoch 1, Batch 1800] loss: 0.178\n",
            "[Epoch 2, Batch 100] loss: 0.146\n",
            "[Epoch 2, Batch 200] loss: 0.145\n",
            "[Epoch 2, Batch 300] loss: 0.147\n",
            "[Epoch 2, Batch 400] loss: 0.157\n",
            "[Epoch 2, Batch 500] loss: 0.153\n",
            "[Epoch 2, Batch 600] loss: 0.143\n",
            "[Epoch 2, Batch 700] loss: 0.126\n",
            "[Epoch 2, Batch 800] loss: 0.121\n",
            "[Epoch 2, Batch 900] loss: 0.152\n",
            "[Epoch 2, Batch 1000] loss: 0.146\n",
            "[Epoch 2, Batch 1100] loss: 0.132\n",
            "[Epoch 2, Batch 1200] loss: 0.150\n",
            "[Epoch 2, Batch 1300] loss: 0.133\n",
            "[Epoch 2, Batch 1400] loss: 0.140\n",
            "[Epoch 2, Batch 1500] loss: 0.100\n",
            "[Epoch 2, Batch 1600] loss: 0.137\n",
            "[Epoch 2, Batch 1700] loss: 0.126\n",
            "[Epoch 2, Batch 1800] loss: 0.127\n",
            "[Epoch 3, Batch 100] loss: 0.098\n",
            "[Epoch 3, Batch 200] loss: 0.107\n",
            "[Epoch 3, Batch 300] loss: 0.103\n",
            "[Epoch 3, Batch 400] loss: 0.106\n",
            "[Epoch 3, Batch 500] loss: 0.122\n",
            "[Epoch 3, Batch 600] loss: 0.118\n",
            "[Epoch 3, Batch 700] loss: 0.099\n",
            "[Epoch 3, Batch 800] loss: 0.116\n",
            "[Epoch 3, Batch 900] loss: 0.115\n",
            "[Epoch 3, Batch 1000] loss: 0.112\n",
            "[Epoch 3, Batch 1100] loss: 0.119\n",
            "[Epoch 3, Batch 1200] loss: 0.116\n",
            "[Epoch 3, Batch 1300] loss: 0.104\n",
            "[Epoch 3, Batch 1400] loss: 0.086\n",
            "[Epoch 3, Batch 1500] loss: 0.105\n",
            "[Epoch 3, Batch 1600] loss: 0.094\n",
            "[Epoch 3, Batch 1700] loss: 0.101\n",
            "[Epoch 3, Batch 1800] loss: 0.114\n",
            "[Epoch 4, Batch 100] loss: 0.081\n",
            "[Epoch 4, Batch 200] loss: 0.079\n",
            "[Epoch 4, Batch 300] loss: 0.096\n",
            "[Epoch 4, Batch 400] loss: 0.071\n",
            "[Epoch 4, Batch 500] loss: 0.075\n",
            "[Epoch 4, Batch 600] loss: 0.078\n",
            "[Epoch 4, Batch 700] loss: 0.076\n",
            "[Epoch 4, Batch 800] loss: 0.104\n",
            "[Epoch 4, Batch 900] loss: 0.094\n",
            "[Epoch 4, Batch 1000] loss: 0.096\n",
            "[Epoch 4, Batch 1100] loss: 0.083\n",
            "[Epoch 4, Batch 1200] loss: 0.098\n",
            "[Epoch 4, Batch 1300] loss: 0.100\n",
            "[Epoch 4, Batch 1400] loss: 0.084\n",
            "[Epoch 4, Batch 1500] loss: 0.081\n",
            "[Epoch 4, Batch 1600] loss: 0.088\n",
            "[Epoch 4, Batch 1700] loss: 0.082\n",
            "[Epoch 4, Batch 1800] loss: 0.094\n",
            "[Epoch 5, Batch 100] loss: 0.067\n",
            "[Epoch 5, Batch 200] loss: 0.058\n",
            "[Epoch 5, Batch 300] loss: 0.068\n",
            "[Epoch 5, Batch 400] loss: 0.078\n",
            "[Epoch 5, Batch 500] loss: 0.063\n",
            "[Epoch 5, Batch 600] loss: 0.073\n",
            "[Epoch 5, Batch 700] loss: 0.067\n",
            "[Epoch 5, Batch 800] loss: 0.078\n",
            "[Epoch 5, Batch 900] loss: 0.083\n",
            "[Epoch 5, Batch 1000] loss: 0.076\n",
            "[Epoch 5, Batch 1100] loss: 0.063\n",
            "[Epoch 5, Batch 1200] loss: 0.082\n",
            "[Epoch 5, Batch 1300] loss: 0.096\n",
            "[Epoch 5, Batch 1400] loss: 0.096\n",
            "[Epoch 5, Batch 1500] loss: 0.075\n",
            "[Epoch 5, Batch 1600] loss: 0.074\n",
            "[Epoch 5, Batch 1700] loss: 0.073\n",
            "[Epoch 5, Batch 1800] loss: 0.067\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 7: Save the Model\n",
        "Save the trained model:"
      ],
      "metadata": {
        "id": "or496iLXVWKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'simple_nn.pth')\n"
      ],
      "metadata": {
        "id": "oVma_8PzVYZI"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 8: Load the Model\n",
        "Load the saved model:"
      ],
      "metadata": {
        "id": "f5mYAg0vVcdg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = SimpleNN()\n",
        "model.load_state_dict(torch.load('simple_nn.pth'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uarq9UVAVfK-",
        "outputId": "e4278c58-24b7-435b-8360-fd1fb4ba8d0a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 9: Evaluate the Model\n",
        "Evaluate the model on the test set:"
      ],
      "metadata": {
        "id": "AdtfoJpHViNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct / total}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXvxn3FNVjEu",
        "outputId": "09f080fd-b6be-4959-aac5-3c1d65b63461"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 97.13%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the model\n",
        "from google.colab import files\n",
        "\n",
        "# Download the model file\n",
        "files.download('simple_nn.pth')\n",
        "\n",
        "print('Model downloaded successfully')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "_rkwLhAZXVb2",
        "outputId": "2f84620b-209d-4cc1-de42-3c5b7a364944"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_2d2760f1-c65e-4f15-9925-36c806b398a6\", \"simple_nn.pth\", 1630240)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This example provides a complete workflow for getting started with PyTorch in Google Colab, from installing PyTorch to training a simple model on the MNIST dataset and saving/loading the trained model."
      ],
      "metadata": {
        "id": "P_fY5MWWVnD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Getting Started with Scikit-learn**"
      ],
      "metadata": {
        "id": "tvH3vHuccS_Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A step-by-step guide for getting started with scikit-learn, complete with a sample code lab. This example will cover installing scikit-learn, loading and preprocessing data, training a simple machine learning model, evaluating the model, and saving/loading the model."
      ],
      "metadata": {
        "id": "xyn7y67ZcbMZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Install scikit-learn\n",
        "First, ensure you have scikit-learn installed in your environment:"
      ],
      "metadata": {
        "id": "EEeYpcy3ci0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBNH7_PtcljQ",
        "outputId": "fb357a80-6f2a-4f65-e1c5-076091781401"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Import Libraries\n",
        "Import the necessary libraries:"
      ],
      "metadata": {
        "id": "cNzupw9dcmjo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib"
      ],
      "metadata": {
        "id": "m4MMQdvJcsbW"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Load and Preprocess Data\n",
        "For this example, we'll use the Iris dataset, which is included in scikit-learn:"
      ],
      "metadata": {
        "id": "R30RJfeFcxe7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "g26pllouczG7"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Train a Simple Machine Learning Model\n",
        "We'll train a Logistic Regression model:"
      ],
      "metadata": {
        "id": "raoOPtE8c4jy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train the model\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "Cf8rznL6dCIQ",
        "outputId": "fb950706-ada3-4dcd-8b5f-6d07d86a1371"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(random_state=42)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=42)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Evaluate the Model\n",
        "Evaluate the model on the test set:"
      ],
      "metadata": {
        "id": "epWghSstdFxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Convert accuracy to percentage\n",
        "accuracy_percentage = accuracy * 100\n",
        "print(f'Accuracy: {accuracy_percentage:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scLcXXYcdITO",
        "outputId": "ca928689-ffa2-4cd6-80b5-3eb0df3fc617"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "joblib.dump(model, 'logistic_regression_model.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5rAgjkjdOuk",
        "outputId": "f658bec4-cb71-4332-a128-d02a77ffb8a0"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['logistic_regression_model.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the model\n",
        "from google.colab import files\n",
        "\n",
        "# Download the model file\n",
        "files.download('logistic_regression_model.pkl')\n",
        "\n",
        "print('Model downloaded successfully')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "bl9G20qEeAKp",
        "outputId": "27657ecc-b85f-4820-862e-42f4928d8191"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_4ae524eb-50be-44b3-8089-051d9366ec3f\", \"logistic_regression_model.pkl\", 991)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model downloaded successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Save the Model\n",
        "Save the trained model using joblib:"
      ],
      "metadata": {
        "id": "_Y7ahLzUdJ5j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 7: Load the Model\n",
        "Load the saved model:"
      ],
      "metadata": {
        "id": "2Iye3bu9dSCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "loaded_model = joblib.load('logistic_regression_model.pkl')\n",
        "\n",
        "# Make predictions with the loaded model\n",
        "loaded_y_pred = loaded_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "loaded_accuracy = accuracy_score(y_test, loaded_y_pred)\n",
        "print(f'Loaded model accuracy: {loaded_accuracy*100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5yBTh7TdUae",
        "outputId": "9ddbc8fe-6266-4cfa-cb48-7b43021399d3"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded model accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction to Keras**"
      ],
      "metadata": {
        "id": "UEm9de90hzzo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A step-by-step code lab for using Keras to build, train, and evaluate a simple neural network on the MNIST dataset.\n",
        "\n",
        "Step 1: Install TensorFlow\n",
        "Ensure you have TensorFlow installed, as Keras is now part of the TensorFlow library:"
      ],
      "metadata": {
        "id": "IB0rpQ9Oh54V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eikzf7xjiBil",
        "outputId": "e2c156cd-266b-45e9-cb69-82b76093eaaa"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Import Libraries\n",
        "Import the necessary libraries:"
      ],
      "metadata": {
        "id": "L-E7Y8whiGdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "7qDonbUAiPGa"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Load and Preprocess Data\n",
        "Load the MNIST dataset and preprocess it:"
      ],
      "metadata": {
        "id": "IpokZhFiiR-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize the data to the range of [0, 1]\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)"
      ],
      "metadata": {
        "id": "PFTgWww7iT4I"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Build the Model\n",
        "Define the architecture of the neural network:"
      ],
      "metadata": {
        "id": "0PJhfUKfiW9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "model = Sequential()\n",
        "\n",
        "# Add layers\n",
        "model.add(Flatten(input_shape=(28, 28)))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))"
      ],
      "metadata": {
        "id": "0Zr2YcUYiZcN"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Compile the Model\n",
        "Set the optimizer, loss function, and metrics:"
      ],
      "metadata": {
        "id": "thB5ozVFibjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "w3k36__IieLa"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Train the Model\n",
        "Train the neural network on the training data:"
      ],
      "metadata": {
        "id": "YkrAZpx5ihlX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXvIpIxyijNd",
        "outputId": "34d6f0e9-24db-4797-baf1-c1a993b0456a"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1500/1500 [==============================] - 10s 6ms/step - loss: 0.2886 - accuracy: 0.9194 - val_loss: 0.1568 - val_accuracy: 0.9538\n",
            "Epoch 2/5\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 0.1303 - accuracy: 0.9624 - val_loss: 0.1207 - val_accuracy: 0.9643\n",
            "Epoch 3/5\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0898 - accuracy: 0.9732 - val_loss: 0.1032 - val_accuracy: 0.9688\n",
            "Epoch 4/5\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.0667 - accuracy: 0.9799 - val_loss: 0.1004 - val_accuracy: 0.9687\n",
            "Epoch 5/5\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0503 - accuracy: 0.9849 - val_loss: 0.0955 - val_accuracy: 0.9712\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7cf2e1827220>"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 7: Evaluate the Model\n",
        "Evaluate the trained model on the test data:"
      ],
      "metadata": {
        "id": "pwPj10fBimAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "accuracy_percentage = accuracy * 100\n",
        "print(f'Accuracy: {accuracy_percentage:.2f}%')"
      ],
      "metadata": {
        "id": "3eg7wtSHipDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 8: Save and Load the Model\n",
        "Save the trained model to a file and load it later for making predictions:"
      ],
      "metadata": {
        "id": "t7X5is7lis6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "model.save('mnist_model.keras')\n",
        "\n",
        "# Load the model\n",
        "loaded_model = tf.keras.models.load_model('mnist_model.keras')\n",
        "\n",
        "# Evaluate the loaded model\n",
        "loaded_loss, loaded_accuracy = loaded_model.evaluate(X_test, y_test)\n",
        "loaded_accuracy_percentage = loaded_accuracy * 100\n",
        "print(f'Loaded Model Accuracy: {loaded_accuracy_percentage:.2f}%')"
      ],
      "metadata": {
        "id": "CSTBTh0ziuq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary\n",
        "\n",
        "This step-by-step code lab demonstrates how to:\n",
        "\n",
        "* Install TensorFlow: Ensure the necessary library is available.\n",
        "* Import Libraries: Import the required TensorFlow and Keras libraries.\n",
        "* Load and Preprocess Data: Use the MNIST dataset and preprocess it.\n",
        "* Build the Model: Define a simple neural network architecture.\n",
        "* Compile the Model: Set the optimizer, loss function, and metrics.\n",
        "* Train the Model: Train the model on the training data.\n",
        "* Evaluate the Model: Evaluate the model on the test data, converting accuracy to a percentage.\n",
        "* Save and Load the Model: Save the trained model to a file and load it for future predictions.\n",
        "\n",
        "Running this code should output the accuracy of the model as a percentage, such as:\n",
        "\n",
        "```\n",
        "Accuracy: 98.12%\n",
        "Loaded Model Accuracy: 98.12%\n",
        "```\n",
        "\n",
        "This code sample uses the MNIST dataset, a standard benchmark for evaluating machine learning models. You can modify the code to use different datasets or model architectures as needed."
      ],
      "metadata": {
        "id": "-ksSL3mPi2aK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Close up**"
      ],
      "metadata": {
        "id": "nplFZk2in9hw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Explore the World of ML Frameworks\n",
        "import webbrowser\n",
        "\n",
        "feeling_adventurous = True\n",
        "\n",
        "if feeling_adventurous:\n",
        "    frameworks = ['TensorFlow', 'PyTorch', 'scikit-learn', 'Keras']\n",
        "    for framework in frameworks:\n",
        "        print(f\"Ready to unleash the power of machine learning? Dive into {framework} and become a model-making maestro!\")\n",
        "\n",
        "        # Open the framework's website\n",
        "        def start_learning(framework):\n",
        "            url = f\"https://www.{framework.lower()}.org/\"\n",
        "            webbrowser.open(url)\n",
        "\n",
        "        start_learning(framework)\n",
        "else:\n",
        "    print('Not feeling adventurous today? No worries, but the world of ML frameworks awaits whenever you are ready!')\n",
        "\n",
        "# Join the ML revolution and connect with developers from around the globe.\n",
        "# Discover tutorials, documentation, and communities that will help you master the art of machine learning!\n",
        "# Warning: May result in excessive learning, coding, and the occasional epiphany!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6u0F2mSn8-r",
        "outputId": "3fcafd01-4c1f-4ab2-ddb2-efd5ae325ea9"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ready to unleash the power of machine learning? Dive into TensorFlow and become a model-making maestro!\n",
            "Ready to unleash the power of machine learning? Dive into PyTorch and become a model-making maestro!\n",
            "Ready to unleash the power of machine learning? Dive into scikit-learn and become a model-making maestro!\n",
            "Ready to unleash the power of machine learning? Dive into Keras and become a model-making maestro!\n"
          ]
        }
      ]
    }
  ]
}